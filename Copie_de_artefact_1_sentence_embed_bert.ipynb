{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de artefact-1-sentence-embed-bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rMXYMxqVT1F"
      },
      "source": [
        "## EXPERIMENT 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdlsTX6dwCsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a474c3-c4fe-40de-e0af-d7d4211d72e6"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 11.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=cf88f061fbf0bbbf18df743c0c51f00a5243e1c9a97cbd5ca7bf682ab4782c25\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFG55uv0wIC9"
      },
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertConfig, BertTokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59xN9RNeGym5"
      },
      "source": [
        "# utility function for getting segments\n",
        "def get_segments(tokens):\n",
        "    #print(\"get_segments\")\n",
        "    #print(tokens)\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id \n",
        "    return (seg_ids)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXgmhB3xG9TY"
      },
      "source": [
        "def get_ids(tokens, tokenizer):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzZpNGiTHAb6"
      },
      "source": [
        "def encode_sentence(sent, tokenizer):\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HtHhqgXHCmE"
      },
      "source": [
        "def get_model(model_string = 'bert-base-uncased'):\n",
        "  config = BertConfig.from_pretrained(model_string, output_hidden_states=True)\n",
        "  model = BertModel.from_pretrained(model_string, config=config)\n",
        "  tokenizer = BertTokenizer.from_pretrained(model_string)\n",
        "  return (model, tokenizer, config)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD9bDBFaHGC5"
      },
      "source": [
        "def get_sentence_embedding(sent, model, tokenizer, config):\n",
        "\n",
        "  tokens = encode_sentence(sent, tokenizer)\n",
        "  segments_idx = get_segments(tokens)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  #print(indexed_tokens)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_idx])\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs  = model(tokens_tensor, segments_tensors)\n",
        "  embeddings_of_last_layer = outputs[0]\n",
        "  cls_embeddings = embeddings_of_last_layer[0]\n",
        "  last_hidden_states = outputs[0] \n",
        "  hidden_states = outputs[2]\n",
        "  embedding_output = hidden_states[0]\n",
        "  encoded_layers = attention_hidden_states = hidden_states[1:]\n",
        "  # BERT has twelve (in this case) layers, we are considering Second Last layer.\n",
        "  #token_vecs = encoded_layers[10][0] # encoded_layers[11][0]\n",
        "  token_vecs = cls_embeddings\n",
        "  sentence_embedding = torch.mean(token_vecs, dim=0) # Calculating average across the sentence.\n",
        "  return(sentence_embedding)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ntn6r_OJedd"
      },
      "source": [
        "def get_document_embedding(lstdocuments, model, tokenizer, config ):\n",
        "  docembeddings = []\n",
        "  for doc in lstdocuments:\n",
        "    docembeddings.append(get_sentence_embedding(doc, model, tokenizer, config))\n",
        "  return(docembeddings)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ybshbgVJkR8"
      },
      "source": [
        "lst_corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'A man is riding a horse.',\n",
        "          'A woman is playing violin.',\n",
        "          'Two men pushed carts through the woods.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'Sachin Tendulkar is a great player.',\n",
        "          'Sholay is an Indian classic film',\n",
        "          'Dog is hunting for food'\n",
        "          ]\n",
        "test_sentences = [\"Cricket is my favourite game.\", \"I like hindi movies.\", \"Cat is looking to eat\"]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfLc6L4sJ1XB"
      },
      "source": [
        "model, tokenizer, config = get_model();\n",
        "test_embeds = get_document_embedding(test_sentences, model, tokenizer, config)\n",
        "doc_embeds = get_document_embedding(lst_corpus, model, tokenizer, config)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezQ04UIFIIq_"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUP6l-m5J8Vh"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "import scipy"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBfNU7F3KYpD"
      },
      "source": [
        "def calculate_distances(query_embedding, document_emdeddings):\n",
        "  distances_c = []  \n",
        "  for docembed in document_emdeddings:\n",
        "    distances_c.append(scipy.spatial.distance.cosine(query_embedding, docembed))\n",
        "  return(distances_c)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5duPJWiKKegX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc4fdaf-1c71-45a4-c18a-f7b7bbb97c2b"
      },
      "source": [
        "closest_n = 3\n",
        "for query, query_embedding in zip(test_sentences, test_embeds):\n",
        "    distances = calculate_distances(query_embedding, doc_embeds)\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop %s most similar sentences in corpus:\\n\" % closest_n)\n",
        "\n",
        "    for idx, distance in results[0:closest_n]:\n",
        "        print(lst_corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Cricket is my favourite game.\n",
            "\n",
            "Top 3 most similar sentences in corpus:\n",
            "\n",
            "A monkey is playing drums. (Score: 0.6063)\n",
            "Sachin Tendulkar is a great player. (Score: 0.6003)\n",
            "A woman is playing violin. (Score: 0.5699)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: I like hindi movies.\n",
            "\n",
            "Top 3 most similar sentences in corpus:\n",
            "\n",
            "A monkey is playing drums. (Score: 0.6127)\n",
            "A man is eating food. (Score: 0.6020)\n",
            "A woman is playing violin. (Score: 0.5986)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Cat is looking to eat\n",
            "\n",
            "Top 3 most similar sentences in corpus:\n",
            "\n",
            "A man is eating food. (Score: 0.6959)\n",
            "Dog is hunting for food (Score: 0.6752)\n",
            "The girl is carrying a baby. (Score: 0.6217)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4D3CSpcOit2"
      },
      "source": [
        "### EXPERIMENT 1A : TESTING BERT FOR WORD CONTEXT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JU9rS9iOpJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3604b2b4-262c-4d27-abc8-244325557e94"
      },
      "source": [
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs  = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "hidden_states = outputs[2]\n",
        "print(len(hidden_states))  # 13\n",
        "\n",
        "embedding_output = hidden_states[0]\n",
        "attention_hidden_states = hidden_states[1:]\n",
        "token_embeddings = torch.stack(attention_hidden_states, dim=0)\n",
        "token_embeddings.size()\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "token_embeddings.size()\n",
        "token_embeddings = token_embeddings.permute(1,0,2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9_4b03PO_PG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsLUHgzyOqi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144f6b41-873c-4cf2-f380-d721dfa8c232"
      },
      "source": [
        "tokenized_text"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'after',\n",
              " 'stealing',\n",
              " 'money',\n",
              " 'from',\n",
              " 'the',\n",
              " 'bank',\n",
              " 'vault',\n",
              " ',',\n",
              " 'the',\n",
              " 'bank',\n",
              " 'robber',\n",
              " 'was',\n",
              " 'seen',\n",
              " 'fishing',\n",
              " 'on',\n",
              " 'the',\n",
              " 'mississippi',\n",
              " 'river',\n",
              " 'bank',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3t63ENKOsG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbeeb28a-bd87-45d7-fff8-69e612189a06"
      },
      "source": [
        "token_vecs_sum = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    \n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: 22 x 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow2LpEOmP5aN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "815ee6c5-2f1e-4849-a8b5-4e1397f3449b"
      },
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
        "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
        "print(\"river bank   \", str(token_vecs_sum[19][:5]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 vector values for each instance of \"bank\".\n",
            "\n",
            "bank vault    tensor([ 3.3596, -2.9805, -1.5421,  0.7065,  2.0031])\n",
            "bank robber   tensor([ 2.7359, -2.5577, -1.3094,  0.6797,  1.6633])\n",
            "river bank    tensor([ 1.5266, -0.8895, -0.5152, -0.9298,  2.8334])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YebT4C3VQHbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7760d407-d453-4918-ece5-7c8e6e1523cb"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Calculate the cosine similarity between the word bank \n",
        "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector similarity for  *similar*  meanings:  0.94\n",
            "Vector similarity for *different* meanings:  0.69\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}